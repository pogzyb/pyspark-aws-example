{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor, GeneralizedLinearRegression, GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1191585 rows in the dataset\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "|Duration|         Start date|Start station number|Member type|\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "|    2762|2017-07-01 00:01:09|               31289|     Casual|\n",
      "|    2763|2017-07-01 00:01:24|               31289|     Casual|\n",
      "|     690|2017-07-01 00:01:45|               31122|     Member|\n",
      "|     134|2017-07-01 00:01:46|               31201|     Member|\n",
      "|     587|2017-07-01 00:02:05|               31099|     Casual|\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML-Example\").getOrCreate()\n",
    "df = spark.read.csv(\"/home/jovyan/data/rides/2017Q3-capitalbikeshare-tripdata.csv\", header=True)\n",
    "df = df.select(['Duration', 'Start date', 'Start station number', 'Member type'])\n",
    "df = df.withColumn('Start station number', df['Start station number'].cast(IntegerType()))\n",
    "print(f'There are {df.count()} rows in the dataset')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 571 rows in the stations\n",
      "+-----------------+------------------+--------------------+\n",
      "|start_station_lat|start_station_long|Start station number|\n",
      "+-----------------+------------------+--------------------+\n",
      "|        39.083673|        -77.149162|               32017|\n",
      "|        39.123513|         -77.15741|               32018|\n",
      "|        38.990249|         -77.02935|               32019|\n",
      "|        39.107709|        -77.152072|               32020|\n",
      "|        38.982456|        -77.091991|               32021|\n",
      "+-----------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations = spark.read.csv(\"/home/jovyan/data/stations/*\", header=True)\n",
    "print(f'There are {stations.count()} rows in the stations')\n",
    "stations = stations.withColumnRenamed('LATITUDE', 'start_station_lat')\n",
    "stations = stations.withColumnRenamed('LONGITUDE', 'start_station_long')\n",
    "stations = stations.withColumn('Start station number', stations['TERMINAL_NUMBER'].cast(IntegerType()))\n",
    "stations = stations.select(['start_station_lat', 'start_station_long', 'Start station number'])\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rides longer than 1.5 hours\n",
    "one_and_a_half_hours = 60 * 60\n",
    "df = df.filter(df['Duration'] <= one_and_a_half_hours)\n",
    "\n",
    "# remove rides shorter than 3 minutes\n",
    "three_minutes = 60 * 3\n",
    "df = df.filter(df['Duration'] >= three_minutes)\n",
    "\n",
    "# remove unknown 'Member type's\n",
    "df = df.filter(df['Member type'] != 'Unknown')\n",
    "\n",
    "# remove non-existent stations\n",
    "df = df.filter(~(df['Start station number'] == 31008) & ~(df['Start station number'] == 32051) & ~(df['Start station number'] == 32034))\n",
    "\n",
    "# make label/target feature\n",
    "df = df.withColumn('label', F.log1p(df.Duration))\n",
    "\n",
    "# rename 'Member type'\n",
    "df = df.withColumnRenamed('Member Type', 'member_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete dataset has 1085389 rows\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "|Start station number|Duration|         Start date|member_type|            label|start_station_lat|start_station_long|\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "|               31289|    2762|2017-07-01 00:01:09|     Casual|7.924072324923417|        38.890544|        -77.049379|\n",
      "|               31289|    2763|2017-07-01 00:01:24|     Casual| 7.92443418488756|        38.890544|        -77.049379|\n",
      "|               31122|     690|2017-07-01 00:01:45|     Member| 6.53813982376767|        38.928893|         -77.03625|\n",
      "|               31099|     587|2017-07-01 00:02:05|     Casual|6.376726947898627|        38.813485|        -77.049468|\n",
      "|               31099|     586|2017-07-01 00:02:06|     Casual|6.375024819828097|        38.813485|        -77.049468|\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(stations, on='Start station number')\n",
    "df = df.withColumn('start_station_long', df['start_station_long'].cast(DoubleType()))\n",
    "df = df.withColumn('start_station_lat', df['start_station_lat'].cast(DoubleType()))\n",
    "print(f'Complete dataset has {df.count()} rows')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in train.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. EDA \n",
    "Exploratory Data Analysis is covered in the `bike-share-eda.ipynb` notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Prediction Pipeline with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Start date\", F.to_timestamp('Start date', 'yyyy-MM-dd HH:mm:ss'))\n",
    "df = df.withColumn(\"day_of_week\", F.dayofweek(\"Start date\"))\n",
    "df = df.withColumn(\"week_of_year\", F.weekofyear(\"Start date\"))\n",
    "df = df.withColumn(\"month\", F.month(\"Start date\"))\n",
    "df = df.withColumn(\"minute\", F.minute(\"Start date\"))\n",
    "df = df.withColumn(\"hour\", F.hour(\"Start date\"))\n",
    "\n",
    "pi = 3.141592653589793\n",
    "\n",
    "df = df.withColumn('sin_day_of_week', F.sin(2 * pi * df['day_of_week'] / 7))\n",
    "df = df.withColumn('sin_week_of_year', F.sin(2 * pi * df['week_of_year'] / 53))\n",
    "df = df.withColumn('sin_month', F.sin(2 * pi * (df['month'] - 1) / 12))\n",
    "df = df.withColumn('sin_minute', F.sin(2 * pi * df['minute'] / 60))\n",
    "df = df.withColumn('sin_hour', F.sin(2 * pi * df['hour'] / 24))\n",
    "\n",
    "df = df.withColumn('cos_day_of_week', F.cos(2 * pi * df['day_of_week'] / 7))\n",
    "df = df.withColumn('cos_week_of_year', F.cos(2 * pi * df['week_of_year'] / 53))\n",
    "df = df.withColumn('cos_month', F.cos(2 * pi * (df['month'] - 1) / 12))\n",
    "df = df.withColumn('cos_minute', F.cos(2 * pi * df['minute'] / 60))\n",
    "df = df.withColumn('cos_hour', F.cos(2 * pi * df['hour'] / 24))\n",
    "\n",
    "# df = df.withColumn('hour_and_day_of_week', df['hour'].cast(StringType()) + '_' + df['day_of_week'].cast(StringType()))\n",
    "# df = df.withColumn('member_type_and_day_of_week', df['member_type'] + '_' + df['day_of_week'].cast(StringType()))\n",
    "\n",
    "df = df.drop(\"Start date\", \"Start station number\", \"Duration\", \"month\", \"hour\", \"minute\", \"day_of_week\", \"week_of_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+--------------------+---------------------------+\n",
      "|member_type|            label|start_station_lat|start_station_long|     sin_day_of_week|   sin_week_of_year|           sin_month|         sin_minute|sin_hour|cos_day_of_week|   cos_week_of_year|cos_month|        cos_minute|cos_hour|hour_and_day_of_week|member_type_and_day_of_week|\n",
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+--------------------+---------------------------+\n",
      "|     Casual|7.924072324923417|        38.890544|        -77.049379|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|                null|                       null|\n",
      "|     Casual| 7.92443418488756|        38.890544|        -77.049379|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|                null|                       null|\n",
      "|     Member| 6.53813982376767|        38.928893|         -77.03625|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|                null|                       null|\n",
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+--------------------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"member_type\" does not exist.\\nAvailable fields: Member type, label, start_station_lat, start_station_long, sin_day_of_week, sin_week_of_year, sin_month, sin_minute, sin_hour, cos_day_of_week, cos_week_of_year, cos_month, cos_minute, cos_hour'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1917.fit.\n: java.lang.IllegalArgumentException: Field \"member_type\" does not exist.\nAvailable fields: Member type, label, start_station_lat, start_station_long, sin_day_of_week, sin_week_of_year, sin_month, sin_minute, sin_hour, cos_day_of_week, cos_week_of_year, cos_month, cos_minute, cos_hour\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:85)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f4a1137f982d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# encoders = [OneHotEncoder(inputCol=ic, outputCol=ic + '_enc')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f4a1137f982d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# encoders = [OneHotEncoder(inputCol=ic, outputCol=ic + '_enc')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"member_type\" does not exist.\\nAvailable fields: Member type, label, start_station_lat, start_station_long, sin_day_of_week, sin_week_of_year, sin_month, sin_minute, sin_hour, cos_day_of_week, cos_week_of_year, cos_month, cos_minute, cos_hour'"
     ]
    }
   ],
   "source": [
    "# encode the categorical feature 'Member type'\n",
    "# rider_indexer = StringIndexer(inputCol='Member type', outputCol='rider_idx')\n",
    "# rider_encoder = OneHotEncoder(inputCol='rider_idx', outputCol='rider_enc')\n",
    "\n",
    "cat_cols = ['member_type', 'hour_and_day_of_week', 'member_type_and_day_of_week']\n",
    "\n",
    "# https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + '_ind').fit(df) for c in cat_cols]\n",
    "# encoders = [OneHotEncoder(inputCol=ic, outputCol=ic + '_enc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VectorAssembler for all features\n",
    "vector = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'start_station_lat',\n",
    "        'start_station_long',\n",
    "        'sin_day_of_week',\n",
    "        'cos_day_of_week',\n",
    "        'sin_week_of_year',\n",
    "        'cos_week_of_year',\n",
    "        'sin_month',\n",
    "        'cos_month',\n",
    "        'sin_minute',\n",
    "        'cos_minute',\n",
    "        'sin_hour',\n",
    "        'cos_hour',\n",
    "    ] + indexers,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# StandardScaler will scale all features\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features', \n",
    "    outputCol='scaled_features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='scaled_features')\n",
    "# glr = GeneralizedLinearRegression(featuresCol='scaled_features')\n",
    "gbt = GBTRegressor(featuresCol='scaled_features')\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        rider_indexer, \n",
    "        rider_encoder, \n",
    "        vector, \n",
    "        scaler,\n",
    "        rf\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation = RegressionEvaluator()\n",
    "grid = ParamGridBuilder()\n",
    "\n",
    "# RandomForest\n",
    "grid = grid.addGrid(rf.numTrees, [50])\n",
    "grid = grid.addGrid(rf.maxDepth, [3, 5])\n",
    "\n",
    "# GLR\n",
    "# grid = grid.addGrid(glr.maxIter, [35])\n",
    "# grid = grid.addGrid(glr.family, ['gamma'])\n",
    "# grid = grid.addGrid(glr.regParam, [0.1])\n",
    "\n",
    "# GBT\n",
    "# grid = grid.addGrid(gtb.maxDepth, [3, 5, 7])\n",
    "# grid = grid.addGrid(gtb.maxIter, [20, 35])\n",
    "\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluation,\n",
    "    numFolds=folds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.7, .3])\n",
    "models = cv.fit(train)\n",
    "best_pipeline = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5835253385131599]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = models.transform(test)\n",
    "evaluation.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_params(params: dict) -> dict:\n",
    "    best_params_as_dict = {}\n",
    "    for param, value in params.items():\n",
    "        param_name = param.name\n",
    "        best_params_as_dict[param_name] = value\n",
    "    return best_params_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cacheNodeIds': False,\n",
       " 'checkpointInterval': 10,\n",
       " 'featureSubsetStrategy': 'auto',\n",
       " 'featuresCol': 'scaled_features',\n",
       " 'impurity': 'variance',\n",
       " 'labelCol': 'label',\n",
       " 'maxBins': 32,\n",
       " 'maxDepth': 5,\n",
       " 'maxMemoryInMB': 256,\n",
       " 'minInfoGain': 0.0,\n",
       " 'minInstancesPerNode': 1,\n",
       " 'numTrees': 50,\n",
       " 'predictionCol': 'prediction',\n",
       " 'seed': -7482846411066979404,\n",
       " 'subsamplingRate': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_algo = best_pipeline.stages[-1]\n",
    "extract_best_params(best_algo.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/model.pmml'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark2pmml import PMMLBuilder\n",
    "pmmlBuilder = PMMLBuilder(spark.sparkContext, df, best_pipeline).putOption(rf, \"compact\", True)\n",
    "pmmlBuilder.buildFile('/home/jovyan/work/bikes.pmml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
