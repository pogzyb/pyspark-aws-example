{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1191585 rows in the dataset\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "|Duration|         Start date|Start station number|Member type|\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "|    2762|2017-07-01 00:01:09|               31289|     Casual|\n",
      "|    2763|2017-07-01 00:01:24|               31289|     Casual|\n",
      "|     690|2017-07-01 00:01:45|               31122|     Member|\n",
      "|     134|2017-07-01 00:01:46|               31201|     Member|\n",
      "|     587|2017-07-01 00:02:05|               31099|     Casual|\n",
      "+--------+-------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML-Example\").getOrCreate()\n",
    "df = spark.read.csv(\"/home/jovyan/data/rides/2017Q3-capitalbikeshare-tripdata.csv\", header=True)\n",
    "df = df.select(['Duration', 'Start date', 'Start station number', 'Member type'])\n",
    "df = df.withColumn('Start station number', df['Start station number'].cast(IntegerType()))\n",
    "print(f'There are {df.count()} rows in the dataset')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 571 rows in the stations\n",
      "+-----------------+------------------+--------------------+\n",
      "|start_station_lat|start_station_long|Start station number|\n",
      "+-----------------+------------------+--------------------+\n",
      "|        39.083673|        -77.149162|               32017|\n",
      "|        39.123513|         -77.15741|               32018|\n",
      "|        38.990249|         -77.02935|               32019|\n",
      "|        39.107709|        -77.152072|               32020|\n",
      "|        38.982456|        -77.091991|               32021|\n",
      "+-----------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations = spark.read.csv(\"/home/jovyan/data/stations/*\", header=True)\n",
    "print(f'There are {stations.count()} rows in the stations')\n",
    "stations = stations.withColumnRenamed('LATITUDE', 'start_station_lat')\n",
    "stations = stations.withColumnRenamed('LONGITUDE', 'start_station_long')\n",
    "stations = stations.withColumn('Start station number', stations['TERMINAL_NUMBER'].cast(IntegerType()))\n",
    "stations = stations.select(['start_station_lat', 'start_station_long', 'Start station number'])\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rides longer than 1.5 hours\n",
    "one_and_a_half_hours = 60 * 60 * 1.5\n",
    "df = df.filter(df['Duration'] <= one_and_a_half_hours)\n",
    "# remove unknown 'Member type's\n",
    "df = df.filter(~(df['Member type'] == 'Unknown'))\n",
    "\n",
    "# remove non-existent stations\n",
    "df = df.filter(~(df['Start station number'] == 31008) & ~(df['Start station number'] == 32051) & ~(df['Start station number'] == 32034))\n",
    "\n",
    "# make label/target feature\n",
    "df = df.withColumn('label', F.log1p(df.Duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete dataset has 1155367 rows\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "|Start station number|Duration|         Start date|Member type|            label|start_station_lat|start_station_long|\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "|               31289|    2762|2017-07-01 00:01:09|     Casual|7.924072324923417|        38.890544|        -77.049379|\n",
      "|               31289|    2763|2017-07-01 00:01:24|     Casual| 7.92443418488756|        38.890544|        -77.049379|\n",
      "|               31122|     690|2017-07-01 00:01:45|     Member| 6.53813982376767|        38.928893|         -77.03625|\n",
      "|               31201|     134|2017-07-01 00:01:46|     Member| 4.90527477843843|         38.90985|        -77.034438|\n",
      "|               31099|     587|2017-07-01 00:02:05|     Casual|6.376726947898627|        38.813485|        -77.049468|\n",
      "+--------------------+--------+-------------------+-----------+-----------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(stations, on='Start station number', how='left')\n",
    "df = df.withColumn('start_station_long', df['start_station_long'].cast(DoubleType()))\n",
    "df = df.withColumn('start_station_lat', df['start_station_lat'].cast(DoubleType()))\n",
    "print(f'Complete dataset has {df.count()} rows')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------+----------+----------------------+-----------------------+\n",
      "|Start station number|max(Start station number)|max(label)|max(start_station_lat)|max(start_station_long)|\n",
      "+--------------------+-------------------------+----------+----------------------+-----------------------+\n",
      "+--------------------+-------------------------+----------+----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col('start_station_lat').isNull()).groupBy('Start station number').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. EDA \n",
    "Exploratory Data Analysis is covered in the `bike-share-eda.ipynb` notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Prediction Pipeline with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Start date\", F.to_timestamp('Start date', 'yyyy-MM-dd HH:mm:ss'))\n",
    "df = df.withColumn(\"day_of_week\", F.dayofweek(\"Start date\"))\n",
    "df = df.withColumn(\"week_of_year\", F.weekofyear(\"Start date\"))\n",
    "df = df.withColumn(\"month\", F.month(\"Start date\"))\n",
    "df = df.withColumn(\"minute\", F.minute(\"Start date\"))\n",
    "df = df.withColumn(\"hour\", F.hour(\"Start date\"))\n",
    "\n",
    "pi = 3.141592653589793\n",
    "\n",
    "df = df.withColumn('sin_day_of_week', F.sin(2 * pi * df['day_of_week'] / 7))\n",
    "df = df.withColumn('sin_week_of_year', F.sin(2 * pi * df['week_of_year'] / 53))\n",
    "df = df.withColumn('sin_month', F.sin(2 * pi * (df['month'] - 1) / 12))\n",
    "df = df.withColumn('sin_minute', F.sin(2 * pi * df['minute'] / 60))\n",
    "df = df.withColumn('sin_hour', F.sin(2 * pi * df['hour'] / 24))\n",
    "\n",
    "df = df.withColumn('cos_day_of_week', F.cos(2 * pi * df['day_of_week'] / 7))\n",
    "df = df.withColumn('cos_week_of_year', F.cos(2 * pi * df['week_of_year'] / 53))\n",
    "df = df.withColumn('cos_month', F.cos(2 * pi * (df['month'] - 1) / 12))\n",
    "df = df.withColumn('cos_minute', F.cos(2 * pi * df['minute'] / 60))\n",
    "df = df.withColumn('cos_hour', F.cos(2 * pi * df['hour'] / 24))\n",
    "\n",
    "df = df.drop(\"Start date\", \"Start station number\", \"Duration\", \"month\", \"hour\", \"minute\", \"day_of_week\", \"week_of_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+\n",
      "|Member type|            label|start_station_lat|start_station_long|     sin_day_of_week|   sin_week_of_year|           sin_month|         sin_minute|sin_hour|cos_day_of_week|   cos_week_of_year|cos_month|        cos_minute|cos_hour|\n",
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+\n",
      "|     Casual|7.924072324923417|        38.890544|        -77.049379|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|\n",
      "|     Casual| 7.92443418488756|        38.890544|        -77.049379|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|\n",
      "|     Member| 6.53813982376767|        38.928893|         -77.03625|-2.44929359829470...|0.05924062789371414|1.224646799147353...|0.10452846326765346|     0.0|            1.0|-0.9982437317643215|     -1.0|0.9945218953682733|     1.0|\n",
      "+-----------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+-------------------+--------+---------------+-------------------+---------+------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical feature 'Member type'\n",
    "rider_indexer = StringIndexer(inputCol='Member type', outputCol='rider_idx')\n",
    "rider_encoder = OneHotEncoder(inputCol='rider_idx', outputCol='rider_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VectorAssembler for all features\n",
    "vector = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'start_station_lat',\n",
    "        'start_station_long',\n",
    "        'rider_enc',\n",
    "        'sin_day_of_week',\n",
    "        'cos_day_of_week',\n",
    "        'sin_week_of_year',\n",
    "        'cos_week_of_year',\n",
    "        'sin_month',\n",
    "        'cos_month',\n",
    "        'sin_minute',\n",
    "        'cos_minute',\n",
    "        'sin_hour',\n",
    "        'cos_hour',\n",
    "    ],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# StandardScaler will scale all features\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features', \n",
    "    outputCol='scaled_features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='scaled_features')\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        rider_indexer, \n",
    "        rider_encoder, \n",
    "        vector, \n",
    "        scaler, \n",
    "        rf\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation = RegressionEvaluator()\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(rf.maxDepth, [5])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluation,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.7, .3])\n",
    "models = cv.fit(train)\n",
    "best = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6696932091328187]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = models.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.668670493022319"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_params(params: dict) -> dict:\n",
    "    best_params_as_dict = {}\n",
    "    for param, value in params.items():\n",
    "        param_name = param.name\n",
    "        best_params_as_dict[param_name] = value\n",
    "    return best_params_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ml = best.stages[-1]\n",
    "# best_ml.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cacheNodeIds': False,\n",
       " 'checkpointInterval': 10,\n",
       " 'featureSubsetStrategy': 'auto',\n",
       " 'featuresCol': 'scaled_features',\n",
       " 'impurity': 'variance',\n",
       " 'labelCol': 'label',\n",
       " 'maxBins': 32,\n",
       " 'maxDepth': 5,\n",
       " 'maxMemoryInMB': 256,\n",
       " 'minInfoGain': 0.0,\n",
       " 'minInstancesPerNode': 1,\n",
       " 'numTrees': 20,\n",
       " 'predictionCol': 'prediction',\n",
       " 'seed': 8675366668351938316,\n",
       " 'subsamplingRate': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_best_params(best.stages[-1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = best.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor_dc842330139b__cacheNodeIds False\n",
      "RandomForestRegressor_dc842330139b__checkpointInterval 10\n",
      "RandomForestRegressor_dc842330139b__featureSubsetStrategy auto\n",
      "RandomForestRegressor_dc842330139b__featuresCol scaled_features\n",
      "RandomForestRegressor_dc842330139b__impurity variance\n",
      "RandomForestRegressor_dc842330139b__labelCol label\n",
      "RandomForestRegressor_dc842330139b__maxBins 32\n",
      "RandomForestRegressor_dc842330139b__maxDepth 5\n",
      "RandomForestRegressor_dc842330139b__maxMemoryInMB 256\n",
      "RandomForestRegressor_dc842330139b__minInfoGain 0.0\n",
      "RandomForestRegressor_dc842330139b__minInstancesPerNode 1\n",
      "RandomForestRegressor_dc842330139b__numTrees 20\n",
      "RandomForestRegressor_dc842330139b__predictionCol prediction\n",
      "RandomForestRegressor_dc842330139b__seed 8675366668351938316\n",
      "RandomForestRegressor_dc842330139b__subsamplingRate 1.0\n"
     ]
    }
   ],
   "source": [
    "for k, v in params.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member type - 0\n",
      "label - 0\n",
      "start_station_lat - 0\n",
      "start_station_long - 0\n",
      "sin_day_of_week - 0\n",
      "sin_week_of_year - 0\n",
      "sin_month - 0\n",
      "sin_minute - 0\n",
      "sin_hour - 0\n",
      "cos_day_of_week - 0\n",
      "cos_week_of_year - 0\n",
      "cos_month - 0\n",
      "cos_minute - 0\n",
      "cos_hour - 0\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(f'{column} - {df.where(F.col(column).isNull()).count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------------+------------------+---------------+----------------+---------+----------+--------+---------------+----------------+---------+----------+--------+\n",
      "|Member type|label|start_station_lat|start_station_long|sin_day_of_week|sin_week_of_year|sin_month|sin_minute|sin_hour|cos_day_of_week|cos_week_of_year|cos_month|cos_minute|cos_hour|\n",
      "+-----------+-----+-----------------+------------------+---------------+----------------+---------+----------+--------+---------------+----------------+---------+----------+--------+\n",
      "+-----------+-----+-----------------+------------------+---------------+----------------+---------+----------+--------+---------------+----------------+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col('start_station_lat').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
