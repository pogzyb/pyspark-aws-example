{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.ml.regression as mlreg\n",
    "import pyspark.ml.tuning as tune\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.pipeline as pipe\n",
    "import pyspark.ml.feature as feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1104418 rows in the dataset\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ML-Example\").getOrCreate()\n",
    "df = spark.read.csv(\"/home/jovyan/data/*\", header=True)\n",
    "print(f'There are {df.count()} rows in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|Duration|         Start date|           End date|Start station number|       Start station|End station number|         End station|Bike number|Member type|\n",
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|     381|2017-04-01 00:00:22|2017-04-01 00:06:43|               31238|      14th & G St NW|             31202|      14th & R St NW|     W22257|     Member|\n",
      "|     590|2017-04-01 00:02:02|2017-04-01 00:11:53|               31109|       7th & T St NW|             31278|      18th & R St NW|     W20006|     Member|\n",
      "|    2938|2017-04-01 00:02:32|2017-04-01 00:51:30|               31289|Henry Bacon Dr & ...|             31238|      14th & G St NW|     W22225|     Casual|\n",
      "|     380|2017-04-01 00:03:02|2017-04-01 00:09:23|               31121|Calvert St & Wood...|             31104|Adams Mill & Colu...|     W20146|     Member|\n",
      "|     423|2017-04-01 00:03:38|2017-04-01 00:10:41|               31023|Fairfax Dr & Wils...|             31034|N Randolph St & F...|     W00316|     Member|\n",
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveMins = 60 * 5\n",
    "threeHours = 60 * 60 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959105"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(df['Duration'] > fiveMins).filter(df['Duration'] < threeHours).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. EDA \n",
    "- Already covered most of the EDA I wanted to in the other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Prediction Pipeline with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformations\n",
    "\n",
    "# - log1p of duration\n",
    "# - convert \"Start date\" to a timestamp and extract: dayofweek, dayofyear, hour, minute\n",
    "# - feature for station\n",
    "# - drop unused columns: \"Start date\", \"End date\", \"Start station\", \"End station number\", \"End station\", \"Duration\", \"Bike number\"\n",
    "\n",
    "df = df.withColumn(\"duration-log1p\", F.log1p(df.Duration))\n",
    "df = df.withColumn(\"Start date\", F.to_timestamp('Start date', 'yyyy-MM-dd HH:mm:ss'))\n",
    "df = df.withColumn(\"day_of_week\", F.dayofweek(\"Start date\"))\n",
    "df = df.withColumn(\"month\", F.dayofyear(\"Start date\"))\n",
    "df = df.withColumn(\"minute\", F.minute(\"Start date\"))\n",
    "df = df.withColumn(\"hour\", F.hour(\"Start date\"))\n",
    "\n",
    "df = df.drop(\"Start date\", \"End date\", \"Start station\", \"End station number\", \"End station\", \"Duration\", \"Bike number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+-----------+-----+------+----+\n",
      "|Start station number|Member type|    duration-log1p|day_of_week|month|minute|hour|\n",
      "+--------------------+-----------+------------------+-----------+-----+------+----+\n",
      "|               31238|     Member| 5.945420608606575|          7|   91|     0|   0|\n",
      "|               31109|     Member|6.3818160174060985|          7|   91|     2|   0|\n",
      "|               31289|     Casual| 7.985824666418917|          7|   91|     2|   0|\n",
      "|               31121|     Member| 5.942799375126701|          7|   91|     3|   0|\n",
      "|               31023|     Member| 6.049733455231958|          7|   91|     3|   0|\n",
      "+--------------------+-----------+------------------+-----------+-----+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical features: 'Member type' & 'Start station number'\n",
    "\n",
    "rider_indexer = feat.StringIndexer(inputCol='Member type', outputCol='rider_idx')\n",
    "rider_encoder = feat.OneHotEncoder(inputCol='rider_idx', outputCol='rider_enc')\n",
    "station_indexer = feat.StringIndexer(inputCol='Start station number', outputCol='station_idx')\n",
    "station_encoder = feat.OneHotEncoder(inputCol='station_idx', outputCol='station_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'label' column (ML libraries in pyspark expect the target feature to be called 'label')\n",
    "df = df.withColumnRenamed('duration-log1p', 'label').drop('duration-log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler - \n",
    "vec = feat.VectorAssembler(\n",
    "    inputCols=['rider_enc', 'station_enc', 'day_of_week', 'month', 'minute', 'hour'],\n",
    "    outputCol='features'\n",
    ")\n",
    "# df = vec.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = mlreg.RandomForestRegressor(maxBins=347)\n",
    "# pipeline = pipe.Pipeline(stages=[rider_indexer, rider_encoder, station_indexer, station_encoder, vec, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piped = pipeline.fit(df).transform(df)\n",
    "# train, test = piped.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.select('features', 'label')\n",
    "# test = test.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = mlreg.RandomForestRegressor()\n",
    "pipeline = pipe.Pipeline(stages=[rider_indexer, rider_encoder, station_indexer, station_encoder, vec, rf])\n",
    "evaluation = evals.RegressionEvaluator()\n",
    "grid = tune.ParamGridBuilder()\n",
    "grid = grid.addGrid(rf.maxDepth, [3, 5])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluation,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.7, .3])\n",
    "models = cv.fit(train)\n",
    "best = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7546669035844837, 0.7394588444471769]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = best.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391634634226683"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
