{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dill\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from scipy import stats\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Create dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in sample_set.csv...\n",
      "Modifying target variable \"Duration\" => \"duration_log1p\"...\n",
      "Removing unreasonably long rides (greater than 1.5 hours) ...\n",
      "Removing \"Unknown\" Member type rows...\n",
      "Cleaned Rides Dataset (rows, columns): (935621, 7)\n"
     ]
    }
   ],
   "source": [
    "seed = 33\n",
    "row_limit = 1000000 \n",
    "data_dir = '../data/'\n",
    "\n",
    "rides_use_columns = [\n",
    "    'Duration', 'Start date', 'End date', 'Start station number', 'End station number', 'Member type'\n",
    "]\n",
    "dataset = pd.DataFrame()\n",
    "for file in os.listdir(os.path.join(data_dir, 'rides')):\n",
    "    if file.startswith('sample_set'):\n",
    "        print(f'Reading in {file}...')\n",
    "        tmp = pd.read_csv(\n",
    "            os.path.join(data_dir, 'rides', file), \n",
    "            nrows=row_limit, \n",
    "            usecols=rides_use_columns\n",
    "        )\n",
    "        dataset = pd.concat([dataset, tmp], sort=False, ignore_index=True)\n",
    "\n",
    "stations_use_columns = [\n",
    "    'TERMINAL_NUMBER', 'LONGITUDE', 'LATITUDE', 'NUMBER_OF_EMPTY_DOCKS', 'NUMBER_OF_BIKES'\n",
    "]\n",
    "stations = pd.read_csv(\n",
    "    os.path.join(data_dir, 'stations/capbs_stations.csv'),\n",
    "    usecols=stations_use_columns\n",
    ")\n",
    "\n",
    "print(f'Modifying target variable \"Duration\" => \"duration_log1p\"...')\n",
    "dataset['duration_log1p'] = np.log1p(dataset.Duration)\n",
    "\n",
    "hours = 1.5\n",
    "mins = 3\n",
    "print(f'Removing unreasonably long rides (greater than {hours} hours) ...')\n",
    "dataset = dataset.loc[(dataset['Duration'] <= (60 * 60 * hours)) & (dataset['Duration'] >= (60 * mins)), :]\n",
    "\n",
    "print(f'Removing \"Unknown\" Member type rows...')\n",
    "dataset = dataset.loc[(dataset['Member type'] != 'Unknown'), :]\n",
    "\n",
    "dataset = dataset.loc[~(dataset['Start station number'] == 31008) &\n",
    "                       ~(dataset['Start station number'] == 32051) &\n",
    "                       ~(dataset['Start station number'] == 32034), :]\n",
    "\n",
    "dataset = dataset.loc[~(dataset['End station number'] == 31008) &\n",
    "                       ~(dataset['End station number'] == 32051) &\n",
    "                       ~(dataset['End station number'] == 32034), :]\n",
    "\n",
    "print(f'Cleaned Rides Dataset (rows, columns): {dataset.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations information (rows, columns): (571, 5)\n",
      "Merged Stations information into Rides dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f'Stations information (rows, columns): {stations.shape}')\n",
    "stations.loc[:,'station_total_bikes'] = stations.loc[:,'NUMBER_OF_BIKES'] + stations.loc[:,'NUMBER_OF_EMPTY_DOCKS']\n",
    "stations_start = stations.loc[:, ['station_total_bikes', 'LATITUDE', 'LONGITUDE', 'TERMINAL_NUMBER']]\n",
    "stations_start.columns = ['station_total_bikes', 'start_station_lat', 'start_station_long', 'Start station number']\n",
    "\n",
    "stations_end = stations.loc[:, ['LATITUDE', 'LONGITUDE', 'TERMINAL_NUMBER']]\n",
    "stations_end.columns = ['end_station_lat', 'end_station_long', 'End station number']\n",
    "\n",
    "dataset = dataset.merge(stations_start, on='Start station number', how='left', sort=False)\n",
    "dataset = dataset.merge(stations_end, on='End station number', how='left', sort=False)\n",
    "print(f'Merged Stations information into Rides dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset (rows, columns): (626866, 12)\n",
      "Testset (rows, columns): (308755, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Start station number</th>\n",
       "      <th>End station number</th>\n",
       "      <th>Member type</th>\n",
       "      <th>duration_log1p</th>\n",
       "      <th>station_total_bikes</th>\n",
       "      <th>start_station_lat</th>\n",
       "      <th>start_station_long</th>\n",
       "      <th>end_station_lat</th>\n",
       "      <th>end_station_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1012</td>\n",
       "      <td>2010-09-20 11:27:04</td>\n",
       "      <td>2010-09-20 11:43:56</td>\n",
       "      <td>31208</td>\n",
       "      <td>31108</td>\n",
       "      <td>Member</td>\n",
       "      <td>6.920672</td>\n",
       "      <td>16</td>\n",
       "      <td>38.876300</td>\n",
       "      <td>-77.003700</td>\n",
       "      <td>38.87670</td>\n",
       "      <td>-77.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1413</td>\n",
       "      <td>2010-09-20 12:10:43</td>\n",
       "      <td>2010-09-20 12:34:17</td>\n",
       "      <td>31100</td>\n",
       "      <td>31201</td>\n",
       "      <td>Member</td>\n",
       "      <td>7.254178</td>\n",
       "      <td>15</td>\n",
       "      <td>38.900300</td>\n",
       "      <td>-77.042900</td>\n",
       "      <td>38.90985</td>\n",
       "      <td>-77.034438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1659</td>\n",
       "      <td>2010-09-20 12:16:36</td>\n",
       "      <td>2010-09-20 12:44:15</td>\n",
       "      <td>31111</td>\n",
       "      <td>31208</td>\n",
       "      <td>Member</td>\n",
       "      <td>7.414573</td>\n",
       "      <td>14</td>\n",
       "      <td>38.917200</td>\n",
       "      <td>-77.025900</td>\n",
       "      <td>38.87630</td>\n",
       "      <td>-77.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1487</td>\n",
       "      <td>2010-09-20 12:19:46</td>\n",
       "      <td>2010-09-20 12:44:34</td>\n",
       "      <td>31703</td>\n",
       "      <td>31603</td>\n",
       "      <td>Member</td>\n",
       "      <td>7.305188</td>\n",
       "      <td>15</td>\n",
       "      <td>38.897063</td>\n",
       "      <td>-76.947446</td>\n",
       "      <td>38.90570</td>\n",
       "      <td>-77.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1007</td>\n",
       "      <td>2010-09-20 12:21:52</td>\n",
       "      <td>2010-09-20 12:38:39</td>\n",
       "      <td>31500</td>\n",
       "      <td>31200</td>\n",
       "      <td>Member</td>\n",
       "      <td>6.915723</td>\n",
       "      <td>15</td>\n",
       "      <td>38.919077</td>\n",
       "      <td>-77.000648</td>\n",
       "      <td>38.91010</td>\n",
       "      <td>-77.044400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Duration           Start date             End date  Start station number  \\\n",
       "0      1012  2010-09-20 11:27:04  2010-09-20 11:43:56                 31208   \n",
       "2      1413  2010-09-20 12:10:43  2010-09-20 12:34:17                 31100   \n",
       "3      1659  2010-09-20 12:16:36  2010-09-20 12:44:15                 31111   \n",
       "4      1487  2010-09-20 12:19:46  2010-09-20 12:44:34                 31703   \n",
       "5      1007  2010-09-20 12:21:52  2010-09-20 12:38:39                 31500   \n",
       "\n",
       "   End station number Member type  duration_log1p  station_total_bikes  \\\n",
       "0               31108      Member        6.920672                   16   \n",
       "2               31201      Member        7.254178                   15   \n",
       "3               31208      Member        7.414573                   14   \n",
       "4               31603      Member        7.305188                   15   \n",
       "5               31200      Member        6.915723                   15   \n",
       "\n",
       "   start_station_lat  start_station_long  end_station_lat  end_station_long  \n",
       "0          38.876300          -77.003700         38.87670        -77.017800  \n",
       "2          38.900300          -77.042900         38.90985        -77.034438  \n",
       "3          38.917200          -77.025900         38.87630        -77.003700  \n",
       "4          38.897063          -76.947446         38.90570        -77.005600  \n",
       "5          38.919077          -77.000648         38.91010        -77.044400  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = dataset.sample(frac=0.33, random_state=seed)\n",
    "dataset.drop(testset.index, inplace=True)\n",
    "print(f'Trainset (rows, columns): {dataset.shape}')\n",
    "print(f'Testset (rows, columns): {testset.shape}')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii Prep features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_metrics = {}\n",
    "\n",
    "# def get_distance(c1_lat_long: Tuple[float, float], c2_lat_long: Tuple[float, float]) -> float:\n",
    "#     return np.sqrt((c1_lat_long[0] - c2_lat_long[0])**2 + (c1_lat_long[1] - c2_lat_long[1])**2)\n",
    "\n",
    "# def get_station_city_location(station_num: int):\n",
    "#     return\n",
    "\n",
    "# def get_average_station_distance(station_num: int):\n",
    "    \n",
    "#     return\n",
    "\n",
    "# def get_median_station_duration(station_num: int):\n",
    "#     return\n",
    "\n",
    "# for row in stations_start.iterrows():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = np.vstack((\n",
    "#     dataset.loc[:,['start_station_long', 'start_station_lat']].values,\n",
    "#     dataset.loc[:,['end_station_long', 'end_station_lat']].values\n",
    "# ))\n",
    "\n",
    "# dataset['week_of_year'] = dataset['Start date'].apply(lambda x: x.weekofyear)\n",
    "# dataset['day_of_week'] = dataset['Start date'].apply(lambda x: x.dayofweek)\n",
    "# dataset['hour'] = dataset['Start date'].apply(lambda x: x.hour)\n",
    "\n",
    "# ride_scaler = StandardScaler()\n",
    "# ride_scaler.fit(dataset.loc[:,['start_station_long', 'start_station_lat', 'hour', 'day_of_week', 'week_of_year']])\n",
    "\n",
    "# ride_scaler.fit(dataset.loc[:,['start_station_long', 'start_station_lat', 'hour', 'day_of_week', 'week_of_year']])\n",
    "\n",
    "\n",
    "# kmeans = MiniBatchKMeans(n_clusters=50, batch_size=100000).fit(rides_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.loc[:, 'ride_cluster'] = kmeans.predict(dataset[['start_station_long', 'start_station_lat']]).astype('str')\n",
    "# testset.loc[:, 'ride_cluster'] = kmeans.predict(testset[['start_station_long', 'start_station_lat']]).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareRiders(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._final_columns = None\n",
    "    \n",
    "    def _make_time_cyclical(self, feature: pd.Series, time_divisor: int) -> Tuple[np.array, np.array]:\n",
    "        sin = np.sin(2 * np.pi * feature / time_divisor)\n",
    "        cos = np.cos(2 * np.pi * feature / time_divisor)\n",
    "        return (sin, cos)\n",
    "        \n",
    "    def transform(self, Xt: pd.DataFrame) -> np.array:\n",
    "        print(f'Initial dataset shape: {Xt.shape}')\n",
    "        \n",
    "        # convert to date type\n",
    "        Xt.loc[:,'Start date'] = pd.to_datetime(Xt['Start date'])\n",
    "        \n",
    "        # separate out date features\n",
    "        Xt.loc[:,'day_of_week'] = Xt['Start date'].apply(lambda x: x.dayofweek)\n",
    "        Xt.loc[:,'week_of_year'] = Xt['Start date'].apply(lambda x: x.weekofyear)\n",
    "        Xt.loc[:,'month'] = Xt['Start date'].apply(lambda x: x.month)\n",
    "        Xt.loc[:,'minute'] = Xt['Start date'].apply(lambda x: x.minute)\n",
    "        Xt.loc[:,'hour'] = Xt['Start date'].apply(lambda x: x.hour)\n",
    "        Xt.drop(['Start date'], axis=1, inplace=True)\n",
    "        \n",
    "        # make date features cyclical\n",
    "        Xt.loc[:,'sin_day_of_week'], Xt.loc[:,'cos_day_of_week'] = self._make_time_cyclical(Xt.loc[:,'day_of_week'], 7)\n",
    "        Xt.loc[:,'sin_week_of_year'], Xt.loc[:,'cos_week_of_year'] = self._make_time_cyclical(Xt.loc[:,'week_of_year'], 53)\n",
    "        Xt.loc[:,'sin_month'], Xt.loc[:,'cos_month'] = self._make_time_cyclical(Xt.loc[:,'month']-1, 12)\n",
    "        Xt.loc[:,'sin_minute'], Xt.loc[:,'cos_minute'] = self._make_time_cyclical(Xt.loc[:,'minute'], 60)\n",
    "        Xt.loc[:,'sin_hour'], Xt.loc[:,'cos_hour'] = self._make_time_cyclical(Xt.loc[:,'hour'], 24)\n",
    "        \n",
    "        # custom transformations\n",
    "        Xt.loc[:, 'hour_and_day_of_week'] = Xt.loc[:,'hour'].astype(str) + '_' + Xt.loc[:,'day_of_week'].astype(str)\n",
    "        Xt.loc[:,'member_type_and_day_of_week'] = Xt.loc[:,'Member type'].astype(str) + '_' + Xt.loc[:,'day_of_week'].astype(str)\n",
    "        \n",
    "        Xt.drop(['day_of_week', 'week_of_year', 'month', 'minute', 'hour'], axis=1, inplace=True)\n",
    "        Xt.drop(['Start station number'], axis=1, inplace=True)\n",
    "        \n",
    "        # transform categorical features\n",
    "        Xt = pd.get_dummies(Xt)\n",
    "        \n",
    "        # save final columns\n",
    "        if not self._final_columns:\n",
    "            self._final_columns = Xt.columns.tolist()\n",
    "        else:\n",
    "            not_in_training = list(set(Xt.columns.tolist()) - set(self._final_columns))\n",
    "            not_in_testing = list(set(self._final_columns) - set(Xt.columns.tolist()))\n",
    "            if not_in_training:\n",
    "                Xt.drop(not_in_training, axis=1, inplace=True)\n",
    "            if not_in_testing:\n",
    "                empties = dict.fromkeys(not_in_testing, 0)\n",
    "                Xt = Xt.assign(**empties)\n",
    "        print(f'Prepared dataset shape: {Xt.shape}')\n",
    "        return Xt.values\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for plotting residuals\n",
    "def fourPlot(y_true: np.array, y_pred: np.array) -> None:\n",
    "    plt.figure(figsize=(15,10));\n",
    "    residuals = y_true - y_pred\n",
    "    # Histogram\n",
    "    plt.subplot(2,2,1);\n",
    "    plt.title(\"Histogram\");\n",
    "    plt.hist(residuals, alpha=0.5);\n",
    "    # Lag plot\n",
    "    plt.subplot(2,2,2);\n",
    "    plt.title(\"Lag Plot\");\n",
    "    lag = residuals.copy()\n",
    "    lag = lag[:-1]\n",
    "    current = residuals[1:]\n",
    "    sns.regplot(current, lag, fit_reg=False);\n",
    "    # QQ plot\n",
    "    plt.subplot(2,2,3);\n",
    "    plt.title(\"QQ Plot\");\n",
    "    qntls, xr = stats.probplot(residuals, fit=False)\n",
    "    sns.regplot(xr, qntls, ci=0);\n",
    "    # Run Sequence plot\n",
    "    plt.subplot(2,2,4);\n",
    "    plt.title(\"Run Sequence\");\n",
    "    sns.regplot(np.arange(len(residuals)), residuals, ci=0);\n",
    "    return\n",
    "\n",
    "# Custom scoring function\n",
    "def rmse_func(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "rmse_scorer = make_scorer(rmse_func, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. RandomizedSearchCV\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randomsearch_with_pipe(\n",
    "        regressors: list, \n",
    "        params_map: dict, \n",
    "        cv: KFold,\n",
    "        df_columns: list,\n",
    "        scorer: callable,\n",
    "        x1: pd.DataFrame, \n",
    "        x2: pd.DataFrame, \n",
    "        y1: np.array, \n",
    "        y2: np.array\n",
    ") -> GridSearchCV:\n",
    "        \n",
    "    for reg, model in regressors:\n",
    "        params = params_map[reg]\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                (\n",
    "                    'prepare', \n",
    "                    PrepareRiders()\n",
    "                ),\n",
    "                (\n",
    "                    'scaler',\n",
    "                    StandardScaler()\n",
    "                ),\n",
    "                (\n",
    "                    reg, \n",
    "                    model(random_state=seed)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        grid = GridSearchCV(estimator=pipe, param_grid=params, scoring=scorer, cv=kf, n_jobs=2)\n",
    "        grid.fit(x1, y1)\n",
    "        # top_features = pd.Series(est.feature_importances_, index=prep._final_columns).sort_values(ascending=False).head(10)\n",
    "        test_preds = grid.predict(x2)\n",
    "        testset_raw_score = np.sqrt(mean_squared_error(y2, test_preds))\n",
    "        testset_rmse_expm1 = np.sqrt(mean_squared_error(np.expm1(y2), np.expm1(test_preds)))\n",
    "        context_error = f'This model is off by ~{round(testset_rmse_expm1 / 60, 2)} minutes'\n",
    "        print(f'Model: {reg}\\nRMSE: {testset_raw_score}\\nRMSE EXPM1: {testset_rmse_expm1}\\nCONTEXT: {context_error}\\nPARAMS: {grid.best_params_}')\n",
    "        # check residuals with the \"Four Plot\"\n",
    "#         print(f'Context error: {context_error}')\n",
    "        \n",
    "        fourPlot(y2, test_preds)\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing variables\n",
    "\n",
    "use_columns = [\n",
    "    'Start date',\n",
    "    'Member type', \n",
    "    'Start station number', \n",
    "    'start_station_lat', \n",
    "    'start_station_long', \n",
    "#     'ride_cluster'\n",
    "]\n",
    "\n",
    "x1, y1 = dataset.loc[:, use_columns], dataset.duration_log1p\n",
    "x2, y2 = testset.loc[:, use_columns], testset.duration_log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n",
      "    obj_ = dumps(obj, reducers=reducers)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n",
      "    dump(obj, buf, reducers=reducers, protocol=protocol)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n",
      "    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 267, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 437, in dump\n",
      "    self.save(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 816, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 771, in save_tuple\n",
      "    save(element)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 771, in save_tuple\n",
      "    save(element)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 662, in save_reduce\n",
      "    save(state)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 786, in save_tuple\n",
      "    save(element)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 816, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 843, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 549, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 638, in save_reduce\n",
      "    save(args)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 771, in save_tuple\n",
      "    save(element)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 504, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 856, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/opt/conda/lib/python3.7/pickle.py\", line 510, in save\n",
      "    rv = reduce(obj)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/_memmapping_reducer.py\", line 340, in __call__\n",
      "    os.chmod(dumped_filename, FILE_PERMISSIONS)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/joblib_memmapping_folder_113_6697736272/113-140434179613920-854b1a5e8f8745ebbcb0a15543eb0688.pkl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/backend/queues.py\", line 175, in _feed\n",
      "    onerror(e, obj)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 310, in _on_queue_feeder_error\n",
      "    self.thread_wakeup.wakeup()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 155, in wakeup\n",
      "    self._writer.send_bytes(b\"\")\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 183, in send_bytes\n",
      "    self._check_closed()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
      "    raise OSError(\"handle is closed\")\n",
      "OSError: handle is closed\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e48e1357dc12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mx2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0my2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-10-4af719dd279a>\u001b[0m in \u001b[0;36mrun_randomsearch_with_pipe\u001b[0;34m(regressors, params_map, cv, df_columns, scorer, x1, x2, y1, y2)\u001b[0m\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# top_features = pd.Series(est.feature_importances_, index=prep._final_columns).sort_values(ascending=False).head(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \n",
    "params_map = {\n",
    "    'rf': {\n",
    "        'rf__max_depth': [3],\n",
    "        'rf__n_estimators': [300],\n",
    "        'rf__min_samples_split': [2],\n",
    "        'rf__min_samples_leaf': [1],\n",
    "        'rf__min_weight_fraction_leaf': [0.0],\n",
    "    },\n",
    "    'gb': {\n",
    "        'gb__max_depth': [5],\n",
    "        'gb__n_estimators': [300],\n",
    "        'gb__learning_rate': [0.1],\n",
    "    },\n",
    "    'ls': {\n",
    "        'ls__alpha': [1.0, 0.5, 0.1],\n",
    "        'ls__max_iter': [1000, 1500]\n",
    "    },\n",
    "    'rd': {\n",
    "        'rd__alpha': [1.0],\n",
    "        'rd__max_iter': [None]\n",
    "    },\n",
    "    'lg': {\n",
    "        'lg__n_estimators': [150],\n",
    "        'lg__num_leaves': [31, 53],\n",
    "        'lg__learning_rate': [0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "regressors = [\n",
    "#     ('rf', RandomForestRegressor),\n",
    "#     ('gb', GradientBoostingRegressor),\n",
    "#     ('ls', Lasso),\n",
    "#     ('rd', Ridge),\n",
    "    ('lg', lgb.LGBMRegressor)\n",
    "]\n",
    "\n",
    "n_splits = 7 # make this smaller if you're impatient; ideally, this should be at least ~10\n",
    "\n",
    "kf = KFold(n_splits=, shuffle=True, random_state=33)\n",
    "\n",
    "grid = run_randomsearch_with_pipe(\n",
    "    regressors=regressors, \n",
    "    params_map=params_map,\n",
    "    df_columns=use_columns,\n",
    "    scorer=rmse_scorer,\n",
    "    cv=kf,\n",
    "    x1=x1,\n",
    "    x2=x2,\n",
    "    y1=y1,\n",
    "    y2=y2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM performs the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning lightgbm ...\n",
    "# Work in progress ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
