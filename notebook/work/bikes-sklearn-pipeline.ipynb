{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dill\n",
    "import os\n",
    "\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from scipy import stats\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Create dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 33\n",
    "row_limit = 1000000 \n",
    "data_dir = '../data/'\n",
    "\n",
    "rides_use_columns = [\n",
    "    'Duration', 'Start date', 'End date', 'Start station number', 'End station number', 'Member type'\n",
    "]\n",
    "dataset = pd.DataFrame()\n",
    "for file in os.listdir(os.path.join(data_dir, 'rides')):\n",
    "    if file.startswith('sample_set'):\n",
    "        print(f'Reading in {file}...')\n",
    "        tmp = pd.read_csv(\n",
    "            os.path.join(data_dir, 'rides', file), \n",
    "            nrows=row_limit, \n",
    "            usecols=rides_use_columns\n",
    "        )\n",
    "        dataset = pd.concat([dataset, tmp], sort=False, ignore_index=True)\n",
    "\n",
    "stations_use_columns = [\n",
    "    'TERMINAL_NUMBER', 'LONGITUDE', 'LATITUDE', 'NUMBER_OF_EMPTY_DOCKS', 'NUMBER_OF_BIKES'\n",
    "]\n",
    "stations = pd.read_csv(\n",
    "    os.path.join(data_dir, 'stations/capbs_stations.csv'),\n",
    "    usecols=stations_use_columns\n",
    ")\n",
    "\n",
    "print(f'Modifying target variable \"Duration\" => \"duration_log1p\"...')\n",
    "dataset['duration_log1p'] = np.log1p(dataset.Duration)\n",
    "\n",
    "hours = 1.5\n",
    "mins = 3\n",
    "print(f'Removing unreasonably long rides (greater than {hours} hours) ...')\n",
    "dataset = dataset.loc[(dataset['Duration'] <= (60 * 60 * hours)) & (dataset['Duration'] >= (60 * mins)), :]\n",
    "\n",
    "print(f'Removing \"Unknown\" Member type rows...')\n",
    "dataset = dataset.loc[(dataset['Member type'] != 'Unknown'), :]\n",
    "\n",
    "dataset = dataset.loc[~(dataset['Start station number'] == 31008) &\n",
    "                       ~(dataset['Start station number'] == 32051) &\n",
    "                       ~(dataset['Start station number'] == 32034), :]\n",
    "\n",
    "dataset = dataset.loc[~(dataset['End station number'] == 31008) &\n",
    "                       ~(dataset['End station number'] == 32051) &\n",
    "                       ~(dataset['End station number'] == 32034), :]\n",
    "\n",
    "print(f'Cleaned Rides Dataset (rows, columns): {dataset.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Stations information (rows, columns): {stations.shape}')\n",
    "stations.loc[:,'station_total_bikes'] = stations.loc[:,'NUMBER_OF_BIKES'] + stations.loc[:,'NUMBER_OF_EMPTY_DOCKS']\n",
    "stations_start = stations.loc[:, ['station_total_bikes', 'LATITUDE', 'LONGITUDE', 'TERMINAL_NUMBER']]\n",
    "stations_start.columns = ['station_total_bikes', 'start_station_lat', 'start_station_long', 'Start station number']\n",
    "\n",
    "stations_end = stations.loc[:, ['LATITUDE', 'LONGITUDE', 'TERMINAL_NUMBER']]\n",
    "stations_end.columns = ['end_station_lat', 'end_station_long', 'End station number']\n",
    "\n",
    "dataset = dataset.merge(stations_start, on='Start station number', how='left', sort=False)\n",
    "dataset = dataset.merge(stations_end, on='End station number', how='left', sort=False)\n",
    "print(f'Merged Stations information into Rides dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset.sample(frac=0.33, random_state=seed)\n",
    "dataset.drop(testset.index, inplace=True)\n",
    "print(f'Trainset (rows, columns): {dataset.shape}')\n",
    "print(f'Testset (rows, columns): {testset.shape}')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii Prep features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_metrics = {}\n",
    "\n",
    "# def get_distance(c1_lat_long: Tuple[float, float], c2_lat_long: Tuple[float, float]) -> float:\n",
    "#     return np.sqrt((c1_lat_long[0] - c2_lat_long[0])**2 + (c1_lat_long[1] - c2_lat_long[1])**2)\n",
    "\n",
    "# def get_station_city_location(station_num: int):\n",
    "#     return\n",
    "\n",
    "# def get_average_station_distance(station_num: int):\n",
    "    \n",
    "#     return\n",
    "\n",
    "# def get_median_station_duration(station_num: int):\n",
    "#     return\n",
    "\n",
    "# for row in stations_start.iterrows():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = np.vstack((\n",
    "#     dataset.loc[:,['start_station_long', 'start_station_lat']].values,\n",
    "#     dataset.loc[:,['end_station_long', 'end_station_lat']].values\n",
    "# ))\n",
    "\n",
    "# dataset['week_of_year'] = dataset['Start date'].apply(lambda x: x.weekofyear)\n",
    "# dataset['day_of_week'] = dataset['Start date'].apply(lambda x: x.dayofweek)\n",
    "# dataset['hour'] = dataset['Start date'].apply(lambda x: x.hour)\n",
    "\n",
    "# ride_scaler = StandardScaler()\n",
    "# ride_scaler.fit(dataset.loc[:,['start_station_long', 'start_station_lat', 'hour', 'day_of_week', 'week_of_year']])\n",
    "\n",
    "# ride_scaler.fit(dataset.loc[:,['start_station_long', 'start_station_lat', 'hour', 'day_of_week', 'week_of_year']])\n",
    "\n",
    "\n",
    "# kmeans = MiniBatchKMeans(n_clusters=50, batch_size=100000).fit(rides_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.loc[:, 'ride_cluster'] = kmeans.predict(dataset[['start_station_long', 'start_station_lat']]).astype('str')\n",
    "# testset.loc[:, 'ride_cluster'] = kmeans.predict(testset[['start_station_long', 'start_station_lat']]).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareRiders(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._final_columns = None\n",
    "    \n",
    "    def _make_time_cyclical(self, feature: pd.Series, time_divisor: int) -> Tuple[np.array, np.array]:\n",
    "        sin = np.sin(2 * np.pi * feature / time_divisor)\n",
    "        cos = np.cos(2 * np.pi * feature / time_divisor)\n",
    "        return (sin, cos)\n",
    "        \n",
    "    def transform(self, Xt: pd.DataFrame) -> np.array:\n",
    "        print(f'Initial dataset shape: {Xt.shape}')\n",
    "        \n",
    "        # convert to date type\n",
    "        Xt.loc[:,'Start date'] = pd.to_datetime(Xt['Start date'])\n",
    "        \n",
    "        # separate out date features\n",
    "        Xt.loc[:,'day_of_week'] = Xt['Start date'].apply(lambda x: x.dayofweek)\n",
    "        Xt.loc[:,'week_of_year'] = Xt['Start date'].apply(lambda x: x.weekofyear)\n",
    "        Xt.loc[:,'month'] = Xt['Start date'].apply(lambda x: x.month)\n",
    "        Xt.loc[:,'minute'] = Xt['Start date'].apply(lambda x: x.minute)\n",
    "        Xt.loc[:,'hour'] = Xt['Start date'].apply(lambda x: x.hour)\n",
    "        Xt.drop(['Start date'], axis=1, inplace=True)\n",
    "        \n",
    "        # make date features cyclical\n",
    "        Xt.loc[:,'sin_day_of_week'], Xt.loc[:,'cos_day_of_week'] = self._make_time_cyclical(Xt.loc[:,'day_of_week'], 7)\n",
    "        Xt.loc[:,'sin_week_of_year'], Xt.loc[:,'cos_week_of_year'] = self._make_time_cyclical(Xt.loc[:,'week_of_year'], 53)\n",
    "        Xt.loc[:,'sin_month'], Xt.loc[:,'cos_month'] = self._make_time_cyclical(Xt.loc[:,'month']-1, 12)\n",
    "        Xt.loc[:,'sin_minute'], Xt.loc[:,'cos_minute'] = self._make_time_cyclical(Xt.loc[:,'minute'], 60)\n",
    "        Xt.loc[:,'sin_hour'], Xt.loc[:,'cos_hour'] = self._make_time_cyclical(Xt.loc[:,'hour'], 24)\n",
    "        \n",
    "        # custom transformations\n",
    "        Xt.loc[:, 'hour_and_day_of_week'] = Xt.loc[:,'hour'].astype(str) + '_' + Xt.loc[:,'day_of_week'].astype(str)\n",
    "        Xt.loc[:,'member_type_and_day_of_week'] = Xt.loc[:,'Member type'].astype(str) + '_' + Xt.loc[:,'day_of_week'].astype(str)\n",
    "        \n",
    "        Xt.drop(['day_of_week', 'week_of_year', 'month', 'minute', 'hour'], axis=1, inplace=True)\n",
    "        Xt.drop(['Start station number'], axis=1, inplace=True)\n",
    "        \n",
    "        # transform categorical features\n",
    "        Xt = pd.get_dummies(Xt)\n",
    "        \n",
    "        # save final columns\n",
    "        if not self._final_columns:\n",
    "            self._final_columns = Xt.columns.tolist()\n",
    "        else:\n",
    "            not_in_training = list(set(Xt.columns.tolist()) - set(self._final_columns))\n",
    "            not_in_testing = list(set(self._final_columns) - set(Xt.columns.tolist()))\n",
    "            if not_in_training:\n",
    "                Xt.drop(not_in_training, axis=1, inplace=True)\n",
    "            if not_in_testing:\n",
    "                empties = dict.fromkeys(not_in_testing, 0)\n",
    "                Xt = Xt.assign(**empties)\n",
    "        print(f'Prepared dataset shape: {Xt.shape}')\n",
    "        return Xt.values\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools for plotting residuals\n",
    "def fourPlot(y_true: np.array, y_pred: np.array) -> None:\n",
    "    plt.figure(figsize=(15,10));\n",
    "    residuals = y_true - y_pred\n",
    "    # Histogram\n",
    "    plt.subplot(2,2,1);\n",
    "    plt.title(\"Histogram\");\n",
    "    plt.hist(residuals, alpha=0.5);\n",
    "    # Lag plot\n",
    "    plt.subplot(2,2,2);\n",
    "    plt.title(\"Lag Plot\");\n",
    "    lag = residuals.copy()\n",
    "    lag = lag[:-1]\n",
    "    current = residuals[1:]\n",
    "    sns.regplot(current, lag, fit_reg=False);\n",
    "    # QQ plot\n",
    "    plt.subplot(2,2,3);\n",
    "    plt.title(\"QQ Plot\");\n",
    "    qntls, xr = stats.probplot(residuals, fit=False)\n",
    "    sns.regplot(xr, qntls, ci=0);\n",
    "    # Run Sequence plot\n",
    "    plt.subplot(2,2,4);\n",
    "    plt.title(\"Run Sequence\");\n",
    "    sns.regplot(np.arange(len(residuals)), residuals, ci=0);\n",
    "    return\n",
    "\n",
    "# Custom scoring function\n",
    "def rmse_func(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "rmse_scorer = make_scorer(rmse_func, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. RandomizedSearchCV\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randomsearch_with_pipe(\n",
    "        regressors: list, \n",
    "        params_map: dict, \n",
    "        cv: KFold,\n",
    "        df_columns: list,\n",
    "        scorer: callable,\n",
    "        x1: pd.DataFrame, \n",
    "        x2: pd.DataFrame, \n",
    "        y1: np.array, \n",
    "        y2: np.array\n",
    ") -> GridSearchCV:\n",
    "        \n",
    "    for reg, model in regressors:\n",
    "        params = params_map[reg]\n",
    "        pipe = Pipeline(\n",
    "            steps=[\n",
    "                (\n",
    "                    'prepare', \n",
    "                    PrepareRiders()\n",
    "                ),\n",
    "                (\n",
    "                    'scaler',\n",
    "                    StandardScaler()\n",
    "                ),\n",
    "                (\n",
    "                    reg, \n",
    "                    model(random_state=seed)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        grid = GridSearchCV(estimator=pipe, param_grid=params, scoring=scorer, cv=kf, n_jobs=2)\n",
    "        grid.fit(x1, y1)\n",
    "        # top_features = pd.Series(est.feature_importances_, index=prep._final_columns).sort_values(ascending=False).head(10)\n",
    "        test_preds = grid.predict(x2)\n",
    "        testset_raw_score = np.sqrt(mean_squared_error(y2, test_preds))\n",
    "        testset_rmse_expm1 = np.sqrt(mean_squared_error(np.expm1(y2), np.expm1(test_preds)))\n",
    "        context_error = f'This model is off by ~{round(testset_rmse_expm1 / 60, 2)} minutes'\n",
    "        print(f'Model: {reg}\\nRMSE: {testset_raw_score}\\nRMSE EXPM1: {testset_rmse_expm1}\\nCONTEXT: {context_error}\\nPARAMS: {grid.best_params_}')\n",
    "        # check residuals with the \"Four Plot\"\n",
    "#         print(f'Context error: {context_error}')\n",
    "        \n",
    "        fourPlot(y2, test_preds)\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing variables\n",
    "\n",
    "use_columns = [\n",
    "    'Start date',\n",
    "    'Member type', \n",
    "    'Start station number', \n",
    "    'start_station_lat', \n",
    "    'start_station_long', \n",
    "#     'ride_cluster'\n",
    "]\n",
    "\n",
    "x1, y1 = dataset.loc[:, use_columns], dataset.duration_log1p\n",
    "x2, y2 = testset.loc[:, use_columns], testset.duration_log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "params_map = {\n",
    "    'rf': {\n",
    "        'rf__max_depth': [3],\n",
    "        'rf__n_estimators': [300],\n",
    "        'rf__min_samples_split': [2],\n",
    "        'rf__min_samples_leaf': [1],\n",
    "        'rf__min_weight_fraction_leaf': [0.0],\n",
    "    },\n",
    "    'gb': {\n",
    "        'gb__max_depth': [5],\n",
    "        'gb__n_estimators': [300],\n",
    "        'gb__learning_rate': [0.1],\n",
    "    },\n",
    "    'ls': {\n",
    "        'ls__alpha': [1.0, 0.5, 0.1],\n",
    "        'ls__max_iter': [1000, 1500]\n",
    "    },\n",
    "    'rd': {\n",
    "        'rd__alpha': [1.0],\n",
    "        'rd__max_iter': [None]\n",
    "    },\n",
    "    'lg': {\n",
    "        'lg__n_estimators': [150],\n",
    "        'lg__num_leaves': [31, 53],\n",
    "        'lg__learning_rate': [0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "regressors = [\n",
    "#     ('rf', RandomForestRegressor),\n",
    "#     ('gb', GradientBoostingRegressor),\n",
    "#     ('ls', Lasso),\n",
    "#     ('rd', Ridge),\n",
    "    ('lg', lgb.LGBMRegressor)\n",
    "]\n",
    "\n",
    "n_splits = 7 # make this smaller if you're impatient; ideally, this should be at least ~10\n",
    "\n",
    "kf = KFold(n_splits=, shuffle=True, random_state=33)\n",
    "\n",
    "grid = run_randomsearch_with_pipe(\n",
    "    regressors=regressors, \n",
    "    params_map=params_map,\n",
    "    df_columns=use_columns,\n",
    "    scorer=rmse_scorer,\n",
    "    cv=kf,\n",
    "    x1=x1,\n",
    "    x2=x2,\n",
    "    y1=y1,\n",
    "    y2=y2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM performs the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning lightgbm ...\n",
    "# Work in progress ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
